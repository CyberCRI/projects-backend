servedUrls:
- '{{ printf ".%s" .Values.hostname }}' # .example.com will match both example.com and www.example.com
- '{{ tpl $.Values.fullName $ }}' # The full name of the k8s service, called from inside the namespace
- '{{ tpl $.Values.fullName $ }}.{{ $.Values.namespace }}' # The full name of the k8s service, called from outside the namespace
- '{{ tpl $.Values.fullName $ }}.{{ $.Values.namespace }}.svc.cluster.local' # Alternative to the full name of the k8s service, called from outside the namespace
- '127.0.0.1' # The localhost, test if this is required
- '.localhost' # The localhost, test if this is required
- '0.0.0.0' # The localhost, test if this is required

additionnalServedUrls: [] # Use this if you want to add custom urls to the list above from the argocd application

corsAllowedDomains: []

config:
  nonSensitive:
    ALLOWED_HOSTS: '{{ tpl (join "," .Values.servedUrls) $ }}'
    CORS_ALLOWED_DOMAINS: '{{ join "," .Values.corsAllowedDomains}}'
    # Kubernetes pods, required for the probes
    POSTGRES_DB: projects
    BEHIND_HTTPS_PROXY: "True"
    ANALYTICS_URL: http://projects-analytics
    MJML_HTTPSERVER_URL: http://mjml-server.mjml/v1/render
    CELERY_BROKER_URL: "redis://redis-backend:6379/0"
    CELERY_RESULT_BACKEND: "redis://redis-backend:6379/0"
    CACHE_REDIS_URL: "redis://redis-backend:6379/1"
    EMAIL_HOST: projects-mailhog
    EMAIL_PORT: "1025"
    # Debug
    DJANGO_CHECK_FAIL_LEVEL: ERROR
    INSTANCE: "{{ .Values.instance }}"
    PUBLIC_URL: "https://{{ .Values.hostname }}"
    PORT: "{{ .Values.backend.port | toString }}"
    GUNICORN_WORKERS_COUNT: "4"
    GUNICORN_MAX_REQUESTS: "1000"
    GUNICORN_MAX_REQUESTS_JITTER: "50"
    GUNICORN_LOG_LEVEL: "info"
    KEYCLOAK_REALM: lp
    VERSION: "{{ .Values.application.revision }}"
    OPENSEARCH_DSL_AUTO_REFRESH: "True"
    OPENSEARCH_DSL_AUTOSYNC: "True"
    OPENSEARCH_USE_SSL: "True"
    OPENSEARCH_USERNAME: projects
    OPENSEARCH_HOST: https://opensearch-cluster-master.opensearch:9200
    OPENSEARCH_INDEX_PREFIX: proj-main

fullName: projects-backend

monitoringNamespace: monitoring

image:
  repository: criprodprod.azurecr.io
  path: projects-backend
  tag: "{{ .Values.application.revision }}"

ingressPath: /
ingressMiddlewares: []

backend:
  replicaCount: 1
  commonLabels:
    app.kubernetes.io/name: projects-backend
    app.kubernetes.io/component: backend
    app.kubernetes.io/part-of: projects
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/version: "{{ .Chart.AppVersion }}"
    app.kubernetes.io/instance: "{{ .Values.instance }}"
  resources:
    requests:
      cpu: 250m
      memory: 170M
    limits:
      memory: 300M
  port: 8000
  horizontalScaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 4
    targetCPUUtilizationPercentage: 50
  networkPolicyAdditionalIngresses:
  - from:
    # Incoming traffic from projects namespace
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: "{{ .Values.namespace }}"
    # Incoming traffic from monitoring namespace
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: "{{ .Values.monitoringNamespace }}"
    ports:
    - protocol: TCP
      port: http

celery:
  enabled: true
  replicaCount: 1
  commonLabels:
    app.kubernetes.io/name: projects-celery
    app.kubernetes.io/component: celery
    app.kubernetes.io/part-of: projects
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/version: "{{ .Chart.AppVersion }}"
    app.kubernetes.io/instance: "{{ .Values.instance }}"
  resources:
    requests:
      cpu: 100m
      memory: 700Mi
    limits:
      memory: 1Gi
  metrics:
    enabled: true
    portNumber: 9808
    image:
      repository: danihodovic
      path: celery-exporter
      tag: "0.10.2"
    brokerUrlIsSecret: false
    livenessProbe:
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 5
      successThreshold: 1
    readinessProbe:
      periodSeconds: 10
      timeoutSeconds: 15
      failureThreshold: 5
      successThreshold: 1
    service:
      enabled: true
      port: 9808
    serviceMonitor:
      enabled: true
      interval: 30s
      scrapeTimeout: 10s

redis:
  enabled: true
  autoSync: false
  valueFiles: []
  argocdAppName: projects-api-redis
  helmReleaseName: projects-api-redis
  targetRevision: main
  cascadeDelete: true

instance: main # By default, the instance of the backend is the main one

workflow:
  resourceName: '{{tpl .Values.fullName $}}-workflow'
  rbac:
    executorClusterRoleName: executor
    serviceAccountName: workflow
  lifecycle:
    enabled: false
    resourceName: '{{ $.Values.fullName }}-lifecycle'
  backups:
    enabled: false
    resourceName: '{{ $.Values.fullName }}-backups'
    slackChannel: '#alerts-prod-error'
    backup:
      cron:
        enabled: false
        schedule: '0 5 * * Tue-Sat' # Backup yesterday's data, so tuesday-saturday to backup monday-friday
    checkPresence:
      cron:
        enabled: false
        schedule: '0 9 * * Tue-Sat' # Backup yesterday's data, so tuesday-saturday to backup monday-friday
    storageAccountName: criparisprodprodbackup
  databaseWorkflowTemplate:
    name: database
    dropDb:
      templateName: drop-db
    createDb:
      templateName: create-db
  opensearch:
    enabled: true
    name: opensearch
    resourceName: opensearch
    createRoleAndUser:
      templateName: create-role-and-user

runMigrations: true

sablier:
  enabled: false
  groupName: projects
  traefikMiddleware: "{{.Values.namespace}}-{{.Values.sablier.groupName}}-sablier@kubernetescrd"

uvicorn:
  entrypoint: projects.asgi:application
  workers: 2
  host: 0.0.0.0
  limitRequests: true
  maxRequests: 1000
